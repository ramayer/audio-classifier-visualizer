{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audioset demos\n",
    "\n",
    "* The AudioSet Strong dataset https://research.google.com/audioset/download_strong.html has nice labels at a sub-second level of granularity.\n",
    "* Need yt-dlp in use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet yt-dlp \n",
    "\n",
    "# %pip install audioset-strong-download # Kinda sucks - If a file is associated to multiple labels, it will be stored multiple times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import IPython.display as ipd\n",
    "import audio_classifier_visualizer as acv\n",
    "\n",
    "try:\n",
    "    import cupy\n",
    "    os.environ['SSQ_GPU'] = '1'\n",
    "    print(\"If this works, cupy is working\",cupy.arange(10))\n",
    "except ImportError as e:\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"For faster performance you may want to `pip install cupy-cuda12x`\")\n",
    "\n",
    "#ddb = duckdb.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import subprocess\n",
    "# import soundfile as sf\n",
    "# import librosa\n",
    "\n",
    "# class AudioSetHelper:\n",
    "#     def __init__(self,\n",
    "#                  output_dir, \n",
    "#                  audio_format='opus', \n",
    "#                  audio_quality='5', \n",
    "#                  sample_rate=16000):\n",
    "#         \"\"\"\n",
    "#         Initializes the AudioSetHelper with common parameters.\n",
    "\n",
    "#         Args:\n",
    "#             output_dir (str): Directory to save the audio files.\n",
    "#             audio_format (str): Audio format (e.g., 'opus', 'mp3', 'wav').\n",
    "#             audio_quality (str): Audio quality (e.g., '5' for Opus, '64K' for MP3).\n",
    "#             sample_rate (int): Sample rate in Hz (e.g., 16000).\n",
    "#         Note:\n",
    "#             Opus seems the best tradeoff.  mpa and aac are slow. mp3 is worse.  wav is huge\n",
    "#         \"\"\"\n",
    "#         self.output_dir = output_dir\n",
    "#         self.audio_format = audio_format\n",
    "#         self.audio_quality = audio_quality\n",
    "#         self.sample_rate = sample_rate\n",
    "\n",
    "#         self._ddb = None\n",
    "#         os.makedirs(self.output_dir, exist_ok=True)\n",
    "\n",
    "    \n",
    "#     def _parse_clip_id(self, clip_id):\n",
    "#         yt_id, start_time_ms = clip_id.split('_')\n",
    "#         start_time_seconds = int(start_time_ms) / 1000  # Convert milliseconds to seconds\n",
    "#         cache_file = os.path.join(self.output_dir, f'{clip_id}.{self.audio_format}')\n",
    "#         return yt_id,start_time_seconds,cache_file\n",
    "\n",
    "#     def download_audio(self, clip_id):\n",
    "#         \"\"\"\n",
    "#         Downloads the entire 10-second audio clip specified by clip_id.\n",
    "\n",
    "#         Args:\n",
    "#             clip_id (str): The clip ID in the format 'ytid_starttimems'.\n",
    "\n",
    "#         Returns:\n",
    "#             tuple: (return_code, stderr) where return_code is the exit status of yt-dlp,\n",
    "#                    and stderr is the error output (if any).\n",
    "\n",
    "#         Note:\n",
    "#             * yt-dlp often ignores -ar\n",
    "#             * between youtube's API, yt-dlp, ffmpeg and librosa, the\n",
    "#             end of the audio clips lose samples, so fetch a little extra\n",
    "#         \"\"\"\n",
    "#         yt_id, st, cache_file = self._parse_clip_id(clip_id)\n",
    "#         et = st + 10+1.1\n",
    "\n",
    "#         _consider_download_sections = \"\"\"\n",
    "#             yt-dlp -x --audio-format {self.format} \n",
    "#             --audio-quality {self.quality} \n",
    "#             --output \"{os.path.join(self.root_path, first_display_label, ytid)}_{start_seconds}-{end_seconds}.%(ext)s\" \n",
    "#             --download-sections *{start_seconds}-{end_seconds} \n",
    "#             --force-keyframes-at-cuts --cookies {self.cookies} https://www.youtube.com/watch?v={ytid}\n",
    "#         \"\"\"\n",
    "#         command = [\n",
    "#             'yt-dlp',\n",
    "#             '-x',  # Extract audio\n",
    "#             '--audio-format', self.audio_format,  # Set audio format\n",
    "#             '--audio-quality', self.audio_quality,  # Set audio quality\n",
    "#             '--postprocessor-args', f'-ss {st} -to {et}  -ar {self.sample_rate}',  # Trim and resample\n",
    "#             '-o', cache_file,  # Output file path\n",
    "#             f'https://www.youtube.com/watch?v={yt_id}'  # YouTube URL\n",
    "#         ]\n",
    "#         process = subprocess.run(command, capture_output=True, text=True)\n",
    "#         if process.returncode:\n",
    "#             with open(cache_file+\".errors\",\"w\") as f:\n",
    "#                 f.write(process.stderr)\n",
    "#         return process.returncode, process.stderr\n",
    "\n",
    "#     def get_audio(self, clip_id):\n",
    "#         _yt_id, _start_time_seconds, cache_file = self._parse_clip_id(clip_id)\n",
    "#         if os.path.exists(cache_file+\".errors\"):\n",
    "#             return None,None\n",
    "#         if not os.path.exists(cache_file):\n",
    "#             rc,se = self.download_audio(clip_id)\n",
    "#             if rc:\n",
    "#                 print(se)\n",
    "#         y, sr = librosa.load(cache_file,mono=True, sr=16000)\n",
    "#         return y, sr\n",
    "    \n",
    "#     def get_youtube_url(self, clip_id):\n",
    "#         yt_id, start_time_seconds, cache_file = self._parse_clip_id(clip_id)\n",
    "#         return f\"https://www.youtube.com/watch?v={yt_id}&t={start_time_seconds}s\"\n",
    "    \n",
    "#     def get_labels_for_a_clip(self,clip_id):\n",
    "#         \"\"\" \n",
    "#             Returns all the audioset labels for a clip as a pandas dataframe like this\n",
    "#             ┌────────────────────┬────────┬────────┬────────────┬───────────────────────┐\n",
    "#             │      clip_id       │   st   │   et   │    mid     │      displayname      │\n",
    "#             │      varchar       │ double │ double │  varchar   │        varchar        │\n",
    "#             ├────────────────────┼────────┼────────┼────────────┼───────────────────────┤\n",
    "#             │ kuoQtqGpsD0_120000 │    0.0 │  0.156 │ /m/0bt9lr  │ Dog                   │\n",
    "#             │ kuoQtqGpsD0_120000 │  0.488 │  2.846 │ /m/01yrx   │ Cat                   │\n",
    "#             │ kuoQtqGpsD0_120000 │  0.737 │  1.062 │ /t/dd00141 │ Pant (dog)            │\n",
    "#             └───────────────────────────────────────────────────────────────────────────┘            \n",
    "#         \"\"\"\n",
    "#         labels_for_a_clip = self.ddb.execute(\"\"\" \n",
    "#             select clip_id,st,et,mid,displayname as lbl\n",
    "#             from source_audioset_train_strong\n",
    "#             join mid_to_display_name using (mid)\n",
    "#             where clip_id = ?\n",
    "#             and displayname != 'Background noise'\n",
    "#             order by st\n",
    "#         \"\"\",(clip_id,))\n",
    "#         return labels_for_a_clip.df()\n",
    "    \n",
    "\n",
    "#     def create_label_arrays(self,clip_id):\n",
    "#         \"\"\"\n",
    "#         Converts labeled time intervals into binary numpy arrays for each label.\n",
    "\n",
    "#         Args:\n",
    "#             df (pd.DataFrame): The DataFrame containing the labeled time intervals.\n",
    "#             clip_duration_ms (int): The duration of the clip in milliseconds (default: 10,000 ms).\n",
    "\n",
    "#         Returns:\n",
    "#             dict: A dictionary where keys are labels and values are binary numpy arrays.\n",
    "#         \"\"\"\n",
    "#         clip_duration_ms = 10 * 1000\n",
    "#         df = self.get_labels_for_a_clip(clip_id)\n",
    "#         # Initialize a dictionary to store the arrays\n",
    "#         label_arrays = {}\n",
    "\n",
    "#         # Iterate through each row in the DataFrame\n",
    "#         for _, row in df.iterrows():\n",
    "#             lbl = row['lbl']\n",
    "#             start_time = row['st']\n",
    "#             end_time = row['et']\n",
    "\n",
    "#             # Convert start and end times to milliseconds\n",
    "#             start_ms = int(start_time * 1000)\n",
    "#             end_ms = int(end_time * 1000)\n",
    "\n",
    "#             # Ensure the label array exists in the dictionary\n",
    "#             if lbl not in label_arrays:\n",
    "#                 label_arrays[lbl] = np.zeros(clip_duration_ms, dtype=int)\n",
    "\n",
    "#             # Set the corresponding elements to 1\n",
    "#             label_arrays[lbl][start_ms:end_ms] = 1\n",
    "\n",
    "#         # return label_arrays\n",
    "\n",
    "#         l = list(label_arrays.keys())\n",
    "#         v = np.stack(list(label_arrays.values()))* 1.0\n",
    "#         v = torch.tensor(v)\n",
    "#         v.shape\n",
    "#         import einx\n",
    "#         v = einx.mean(\"a (b c) -> a b\", v, c=100).T\n",
    "#         v.shape\n",
    "#         return l,v\n",
    "\n",
    "    \n",
    "#     @property\n",
    "#     def ddb(self):\n",
    "#         if self._ddb:\n",
    "#             return self._ddb\n",
    "#         ddb = duckdb.connect(self.output_dir+\"/audioset.ddb\")\n",
    "#         base_uri   = \"http://storage.googleapis.com/us_audioset/youtube_corpus/strong/\"\n",
    "#         ddb.read_csv(base_uri + \"mid_to_display_name.tsv\", \n",
    "#                      header=False,\n",
    "#                      names=['mid','displayname']\n",
    "#                      ).create_view(\"source_mid_to_display_name\")\n",
    "#         ddb.read_csv(base_uri + \"audioset_train_strong.tsv\",\n",
    "#                      header=True,\n",
    "#                      names=['clip_id','st','et','mid']\n",
    "#                      ).create_view(\"source_audioset_train_strong\")\n",
    "        \n",
    "\n",
    "#         ddb.read_csv('http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/class_labels_indices.csv'\n",
    "#                      ).create_view('class_labels_indices')\n",
    "\n",
    "#         # ddb.read_csv('http://storage.googleapis.com/us_audioset/youtube_corpus/v1/csv/balanced_train_segments.csv',skiprows=2) # v1\n",
    "        \n",
    "#         ddb.sql(\"create table if not exists mid_to_display_name as select * from source_mid_to_display_name\")\n",
    "#         ddb.sql(\"create table if not exists audioset_train_strong as select * from source_audioset_train_strong\")\n",
    "#         self._ddb = ddb\n",
    "#         return ddb\n",
    "    \n",
    "#     def _refresh_cookies(self):\n",
    "#         if not os.path.exists('/tmp/cookies.txt'):\n",
    "#             subprocess.run(\"\"\"\n",
    "#                 yt-dlp --cookies-from-browser chromium:~/snap/chromium/common/chromium/Default --cookies /tmp/cookies.txt 0\n",
    "#             \"\"\",shell=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ash = acv.AudioSetHelper(output_dir=\"data/audioset/opus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ash.create_label_arrays('b0RFKhbpFJA_30000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ash.ddb.sql(\" select * from class_labels_indices\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ash.ddb.sql(\"select * from mid_to_display_name limit 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ash.ddb.sql(\"select * from audioset_train_strong limit 3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_labels = ash.ddb.sql(\"select * from mid_to_display_name where displayname in ('Cat','Caterwaul','Purr','Meow','Cat communication')\")\n",
    "dog_labels = ash.ddb.sql(\"select * from mid_to_display_name where displayname ilike '%dog%' or displayname ilike '%bark%' or displayname ilike '%yip%' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "\n",
    "ash.ddb.sql(\"\"\"\n",
    "    with cat_vids as (\n",
    "            select distinct clip_id from audioset_train_strong where mid in (select distinct mid from cat_labels)\n",
    "    ),\n",
    "    dog_vids as (\n",
    "            select distinct clip_id from audioset_train_strong where mid in (select distinct mid from dog_labels)\n",
    "    )\n",
    "    select count(distinct mid) num_labels, clip_id, array_agg(distinct displayname)\n",
    "        from audioset_train_strong as ats\n",
    "        join cat_vids using (clip_id)\n",
    "        join dog_vids using (clip_id)\n",
    "        join mid_to_display_name using (mid)\n",
    "        group by clip_id\n",
    "        order by num_labels desc\n",
    "        limit 10\n",
    "\"\"\").df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip_id = 'kuoQtqGpsD0_120000'\n",
    "print(ash.get_youtube_url(clip_id))\n",
    "y,sr = ash.get_audio(clip_id)\n",
    "y.shape,sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Audioset strong labels are at spacing of 960ms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_a_clip = ash.ddb.execute(\"\"\" \n",
    "    select clip_id,st,et,mid,displayname\n",
    "    from source_audioset_train_strong\n",
    "    join mid_to_display_name using (mid)\n",
    "    where clip_id = ?\n",
    "    and displayname != 'Background noise'\n",
    "    order by st\n",
    "\"\"\",(clip_id,))\n",
    "\n",
    "ash.ddb.sql(f\"\"\" \n",
    "    select clip_id,st,et,mid,displayname\n",
    "    from source_audioset_train_strong\n",
    "    join mid_to_display_name using (mid)\n",
    "    where clip_id = '{clip_id}'\n",
    "    and displayname != 'Background noise'\n",
    "    order by st\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import einx\n",
    "\n",
    "# def create_label_arrays(df, clip_duration_ms=10000):\n",
    "#     \"\"\"\n",
    "#     Converts labeled time intervals into binary numpy arrays for each label.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The DataFrame containing the labeled time intervals.\n",
    "#         clip_duration_ms (int): The duration of the clip in milliseconds (default: 10,000 ms).\n",
    "\n",
    "#     Returns:\n",
    "#         dict: A dictionary where keys are labels and values are binary numpy arrays.\n",
    "#     \"\"\"\n",
    "#     # Initialize a dictionary to store the arrays\n",
    "#     label_arrays = {}\n",
    "\n",
    "#     # Iterate through each row in the DataFrame\n",
    "#     for _, row in df.iterrows():\n",
    "#         lbl = row['lbl']\n",
    "#         start_time = row['st']\n",
    "#         end_time = row['et']\n",
    "\n",
    "#         # Convert start and end times to milliseconds\n",
    "#         start_ms = int(start_time * 1000)\n",
    "#         end_ms = int(end_time * 1000)\n",
    "\n",
    "#         # Ensure the label array exists in the dictionary\n",
    "#         if lbl not in label_arrays:\n",
    "#             label_arrays[lbl] = np.zeros(clip_duration_ms, dtype=int)\n",
    "\n",
    "#         # Set the corresponding elements to 1\n",
    "#         label_arrays[lbl][start_ms:end_ms] = 1\n",
    "\n",
    "#     # return label_arrays\n",
    "\n",
    "#     l = list(label_arrays.keys())\n",
    "#     v = np.stack(list(label_arrays.values()))* 1.0\n",
    "#     v = torch.tensor(v)\n",
    "#     v.shape\n",
    "#     import einx\n",
    "#     v = einx.mean(\"a (b c) -> a b\", v, c=100).T\n",
    "#     v.shape\n",
    "#     return l,v\n",
    "\n",
    "    \n",
    "# # Example usage\n",
    "# data = {\n",
    "#     'clip_id': ['kuoQtqGpsD0_120000', 'kuoQtqGpsD0_120000', 'kuoQtqGpsD0_120000', 'kuoQtqGpsD0_120000'],\n",
    "#     'st': [0.000, 0.488, 0.737, 7.970],\n",
    "#     'et': [0.156, 2.846, 1.062, 10.000],\n",
    "#     'lbl': ['/m/0bt9lr', '/m/01yrx', '/t/dd00141', '/m/01yrx'],\n",
    "#     'displayname': ['Dog', 'Cat', 'Pant (dog)', 'Cat']\n",
    "# }\n",
    "\n",
    "# # Create a DataFrame\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Generate the label arrays\n",
    "# label_arrays = create_label_arrays(df)\n",
    "\n",
    "# # Print the arrays\n",
    "# for lbl, array in label_arrays.items():\n",
    "#     print(f\"Label: {lbl}\")\n",
    "#     print(array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_for_a_clip = ash.ddb.execute(\"\"\" \n",
    "    select clip_id,st,et,mid,displayname as lbl\n",
    "    from source_audioset_train_strong\n",
    "    join mid_to_display_name using (mid)\n",
    "    where clip_id = ?\n",
    "    and displayname != 'Background noise'\n",
    "    order by st\n",
    "\"\"\",(clip_id,))\n",
    "df = labels_for_a_clip.df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l,v = ash.create_label_arrays(clip_id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l = list(label_arrays.keys())\n",
    "# v = np.stack(list(label_arrays.values()))* 1.0\n",
    "# v = torch.tensor(v)\n",
    "# v.shape\n",
    "# import einx\n",
    "# v = einx.mean(\"a (b c) -> a b\", v, c=100).T\n",
    "# v.shape\n",
    "# #v = torch.nn.functional.softmax(v*100, dim=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.shape[0] /sr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# v = torch.tensor([[1.0,0.0],[1.0,0.0]])\n",
    "# l = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_processor = acv.AudioFileProcessor()\n",
    "from audio_classifier_visualizer.audio_file_visualizer import Subplot\n",
    "\n",
    "# TODO: There's a bug scaling the colors on the spectrograms when the duration of y is greater than the duration of the labels.\n",
    "\n",
    "audio_file_visualizer = acv.AudioFileVisualizer(y=y[0:sr*10],\n",
    "                                                sr=sr,\n",
    "                                                start_time=0, \n",
    "                                                end_time=15,\n",
    "                                                feature_rate=10,\n",
    "                                                class_probabilities=v,\n",
    "                                                n_fft=512,\n",
    "                                                class_labels=l,\n",
    "                                                #freq_range_of_interest=(100,12000)\n",
    "                                                )\n",
    "\n",
    "\n",
    "title = f\"AudioSet {clip_id} -- cat-sound vs dog-sound\"\n",
    "audio_file_visualizer.visualize_audio_file_fragment(title, width=12, height=8).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_file_processor = acv.AudioFileProcessor()\n",
    "\n",
    "audio_file_visualizer = acv.AudioFileVisualizer(y=y,\n",
    "                                                sr=sr,\n",
    "                                                start_time=0, \n",
    "                                                end_time=15,\n",
    "                                                feature_rate=1,\n",
    "                                                class_probabilities=torch.tensor([[1.0,0.0],[1.0,0.0]]),\n",
    "                                                n_fft=512,\n",
    "                                                class_labels=[0,0],\n",
    "                                                freq_range_of_interest=(100,12000)\n",
    "                                                )\n",
    "\n",
    "title = clip_id\n",
    "audio_file_visualizer.visualize_audio_file_fragment(title, width=12, height=8).show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "audio-classifier-visualizer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
